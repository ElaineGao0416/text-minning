#错别字纠正函数
作者：Zhiping Gao  
备注：以下错别字识别采用的是词频、拼音、次序（word2vec）综合判断的非监督错别字识别工具，符合的是特殊业务环境，且封装的程度较高。如果需要配合其他文本，需要修改内部的参数`a_num`, `b_num`, `pinyin`, `simi`等。

##函数代码

```
import pypinyin
import jieba
import jieba.posseg as pseg
jieba.initialize()
from thulac import thulac
from collections import Counter
from gensim.models.word2vec import Word2Vec
import codecs
import os
import re
import itertools
import sys
reload(sys)
sys.setdefaultencoding('utf-8')



def thulac_find_rspell(text,path='/Users/zp/Desktop/08test',seg_only=False, filt=True,size=600, window=6, min_count=1, workers=4,sg=1):
	'''thulac分词方法的错别字识别'''
	thu = thulac(user_dict=None, model_path=None, T2S=False, seg_only=seg_only, filt=filt)
	model = Word2Vec(text.encode("utf8"), size=size, window=window, min_count=min_count, workers=workers,sg=sg)
	##分词	
	all_words = []
	only_word = []
	words = thu.cut(text.encode("utf8"))
	for i in words:
		all_words.append((i[0],i[1]))
		only_word.append(i[0])
	cnt_word = Counter(only_word)
	#笛卡尔积
	sample = list(set(all_words))
	for i in itertools.product(range(len(sample)), range(len(sample))):
		if i[0] < i[1]:
			if sample[i[0]][1] != 'ns' and sample[i[0]][1] != 'nr' and sample[i[1]][1] != 'ns' and sample[i[1]][1] != 'nr':
				a = sample[i[0]][0].decode('utf8')
				b = sample[i[1]][0].decode('utf8')
				unicode = fuzz.token_set_ratio(a,b)
				#拼音相似度
				c = " ".join(lazy_pinyin(a))
				d = " ".join(lazy_pinyin(b))
				pinyin1 = fuzz.ratio(c,d)
				pinyin = fuzz.token_set_ratio(c,d)
				if cnt_word[a.encode('utf8')] < 4:
					if len(a) == len(b)  and len(a) > 1 and a != b :
	 					if unicode == 0 and pinyin > 92 and pinyin1 > 94:
	 						#word2vec相似度
	 						simi = model.wv.n_similarity(a.encode('utf8'), b.encode('utf8'))
	 						if simi > 0.5:
	 							#词频
	 							a_num = cnt_word[a.encode('utf8')]
	 							b_num = cnt_word[b.encode('utf8')]
								print a,a_num,b,b_num
								print simi
								if a_num < b_num:
									write_file(path=path,wrong_word=a,ww_num=a_num,right_word=b,rw_num=b_num,simi=simi)
								else:
									write_file(path=path,wrong_word=b,ww_num=b_num,right_word=a,rw_num=a_num,simi=simi)
		


def jieba_fine_rspell(text,path='/Users/zp/Desktop/08test1',size=600, window=8, min_count=1, workers=7,sg=0):
	'''jieba分词方法的错别字识别'''
	##分词
	model = Word2Vec(text.encode("utf8"), size=size, window=window, min_count=min_count, workers=workers,sg=sg)	
	words = pseg.cut(text)
	all_words = []
	only_word = []
	for word,flag in words:
		all_words.append((word,flag))
		only_word.append(word)
	cnt_word = Counter(only_word)
	#笛卡尔积
	sample = list(set(all_words))
	for i in itertools.product(range(len(sample)), range(len(sample))):
		if i[0] < i[1]:
			if sample[i[0]][1] != 'ns' and sample[i[0]][1] != 'nr' \
			and sample[i[1]][1] != 'ns' and sample[i[1]][1] != 'nr' and sample[i[0]][1] != 'm'  and sample[i[1]][1] != 'm':
				a = sample[i[0]][0]
				b = sample[i[1]][0]
				unicode = fuzz.token_set_ratio(a.encode('utf8'),b.encode('utf8'))
				c = " ".join(lazy_pinyin(a))
				d = " ".join(lazy_pinyin(b))
				#拼音相似度
				pinyin1 = fuzz.ratio(c,d)
				pinyin = fuzz.token_set_ratio(c,d)
				if cnt_word[a] < 4:
					if len(a) == len(b)  and len(a) == 2 and a != b :
	 					if unicode == 0 and pinyin > 92 and pinyin1 > 94:
	 						#word2vec相似度
	 						simi = model.wv.n_similarity(a.encode('utf8'), b.encode('utf8'))
	 						if simi > 0.5:
	 							#词频
								a_num = cnt_word[a]
	 							b_num = cnt_word[b]
								print a,a_num,b,b_num
								print simi
								if a_num < b_num:
									write_file(path=path,wrong_word=a,ww_num=a_num,right_word=b,rw_num=b_num,simi=simi)
								else:
									write_file(path=path,wrong_word=b,ww_num=b_num,right_word=a,rw_num=a_num,simi=simi)
					elif len(a) == len(b)  and len(a) > 2 and a != b :
						if unicode == 0 and pinyin > 94 and pinyin1 > 97:
	 						simi = model.wv.n_similarity(a.encode('utf8'), b.encode('utf8'))
	 						if simi > 0.7:
	 							#词频
	 							a_num = cnt_word[a]
	 							b_num = cnt_word[b]
								print a,a_num,b,b_num
								print simi
								if a_num < b_num:
									write_file(path=path,wrong_word=a,ww_num=a_num,right_word=b,rw_num=b_num,simi=simi)
								else:
									write_file(path=path,wrong_word=b,ww_num=b_num,right_word=a,rw_num=a_num,simi=simi)
	


def write_file(path,wrong_word,ww_num,right_word,rw_num,simi):
	'''错别字词库写入'''
	file = open(path,"a+")
	file.write(wrong_word+'|'+str(ww_num)+'|'+right_word+'|'+str(rw_num)+'|'+str(simi)+'\n')
	file.close()


def define_wrong_spell(old_word,new_word,text):
	'''错别字人工识别及文档更新'''
	p = old_word
	word1 = re.compile(r'\，[^，]+?'+p+'.+?\，')
	sentence1 = word1.findall(text)
	if len(sentence1) == 0:
		word1 = re.compile(r'.+?'+p+'.+?\，')
		sentence1 = word1.findall(text)
	for i in sentence1:
		print i[3:-3] 
		defination = raw_input("Is it right spelled ? (y/n) : ")
		while defination != 'y' and defination != 'n' :
			print "Cannot understand your input!"
			defination = raw_input("Is it right spelled ? (y/n) : ")
		if defination == 'n':
			new = i.replace(old_word,new_word)
			text = text.replace(i,new)
		else:
			continue
	return text



def find_wrong_spell(path,text):
	'''利用错别字词库更新文档'''
	data = open(path,'r').readlines()
	for line in data:
		ln = line.strip().split("|")
		print 'wrong_word: '+ln[0]
		print 'possible word: '+ln[2]
		text = define_wrong_spell(ln[0],ln[2],text)
	return text



def load_address_dict(address_dict):
	'''导入地理位置字典'''
	files = os.listdir(address_dict)
	for i in files:
		full_address_dict = address_dict+i
		try:
			jieba.load_userdict(full_address_dict)
		except:
			print full_address_dict
			print "file not load!"
			
```
##函数应用

```
#样本数据
text = codecs.open('/Users/zp/Desktop/nlp/SHARE_NLP/07','r','utf-8').read().encode('utf8')
text = u'未来的事物总会有所变化，今天，我就来给大家介绍一下未来的智能汽车。\
　　未来的汽车华丽而又实用，是驾驶人不可离开的交通工具，未来的汽车一律都是一个牌子的——保时捷，瞧，我爸爸要开车啦!\
　　我和爸爸坐上车，按了一下一个蓝色的按钮，汽车就启动了，从导航里传出优雅的女声：“主人你好，请问要去何处?”“去陡水湖。”爸爸对导航说，\
	立刻，我们的座椅往后拉到最低，安全带自动系上，原来导航可以自动驾驶啊，到了目的地，我们还没察觉，叮——，闹钟声把我们叫醒了，我们下了车。\
　　要回家了，导航又自动返回。\
　　嘻嘻，未来的汽车怎么样啊?'


#定义字典
#jieba分词专用
renewable_dict = '/Users/zp/Desktop/nlp/SHARE_NLP/dict.txt'
address_dict = '/Users/zp/Desktop/nlp/SHARE_NLP/address_dict/'
jieba_stop_word = '/Users/zp/Desktop/nlp/SHARE_NLP/stopword.txt'

#导入辞典
#jieba分词专用
jieba.load_userdict(renewable_dict)
load_address_dict(address_dict)
jieba.analyse.set_stop_words(jieba_stop_word)

#生成错别字词库
thulac_find_rspell(text,path='/Users/zp/Desktop/08test',seg_only=False, filt=True,size=600, window=6, min_count=1, workers=4,sg=1)
jieba_fine_rspell(text,path='/Users/zp/Desktop/08test1',size=600, window=8, min_count=1, workers=7,sg=0)

#文档检查错别字及更新
text = find_wrong_spell(path='/Users/zp/Desktop/08test',text=text)
```

##函数评价
thulac的分词比较缓慢但是准确率高；jieba的分词速度快，但是准确率略低。  
但是jieba可以添加自定义字典，虽然thulac也可以添加，但是由于文档说明太少，不明确如何添加自定义字典。

##参数说明
`text`：sample；   
`path`：错别字词库的路径；  
`seg_only`：boolean，是否保留词性标注；    
`filt`：boolean，是否剔除停用词或噪音词；  
`size`：word2vec的向量长度。越大精确度越高，计算时间越长，但是到达一点程度后，精确程度稳定；     
`window`：word2vec中，the maximum distance between the current and predicted word within a sentence；  
`min_count`:word2vec中，ignore all words with total frequency lower than this；  
`sg`：word2vec中，defines the training algorithm. By default (`sg=0`), CBOW is used. Otherwise (`sg=1`), skip-gram is employed;   
`workers`：word2vec中，use this many worker threads to train the model (=faster training with multicore machines);  

其他关于work2vec的参数，可以自行调整，这里仅设计这些，以提高封装程度。



##样例
###默认词频较小的为错别字

词1|词频1|词2|词频2|w2v相似度
---|---|---|---|---
给于处理|1|给予处理|3|0.934575016633
其它|3|其他|57|0.590693517575
意向|1|已向|1|0.552965581675
欺诈|1|期诈|1|0.655063995163
疑为|3|以为|9|0.611389804509
那里|1|哪里|3|0.560819674357
超时|2|超市|6|0.585325801227
无限|1|无线|4|0.511032264379
不足|2|补足|12|0.574217676724
累积|1|累计|2|0.653717162657
